{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlWb_GHYIJMc"
   },
   "source": [
    "# __Machine Translation from Spanish to English__\n",
    "* Architecture Used: GRU’s\n",
    "* Programming Languages, Libraries, and Frameworks: Python 3.x, NumPy, Matplotlib, sklearn, TensorFlow 2.x, Keras[Tensorflow Backend], unicodedata, os, io, time\n",
    "\n",
    "* __Dataset__:-\n",
    "manythings.org is currently the best website that has the largest collection of Tab-delimited Bilingual Sentence Pairs. It has Tab-delimited Bilingual Sentence Pairs for over 30 different languages. The dataset contains the language-translation pairs in the format:-\n",
    "“English + TAB + The Other Language + TAB + Attribution”\n",
    "This website only provides only the datasets to translate between “ English and the Other Language”. manythings.org is not the only website, there are many other websites from which you can download the dataset of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kDQhmkaIJMd"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:19.072832Z",
     "iopub.status.busy": "2022-12-15T08:11:19.072457Z",
     "iopub.status.idle": "2022-12-15T08:11:19.079949Z",
     "shell.execute_reply": "2022-12-15T08:11:19.078950Z",
     "shell.execute_reply.started": "2022-12-15T08:11:19.072801Z"
    },
    "id": "p6CTnzXwIJMe"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata \n",
    "#This module provides access to the Unicode Character Database (UCD) which\n",
    "# defines character properties for all Unicode characters\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8A6GX0yIJMf"
   },
   "source": [
    "## Preprocessing\n",
    "1. Add a 'start' and 'end' token.\n",
    "2. Removing the special characters.\n",
    "3. Creating a dictionary mapping from word → id and id →word\n",
    "4. Representing each sentence in its vectorized form.\n",
    "5. Padding each Sentence to a maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:19.088741Z",
     "iopub.status.busy": "2022-12-15T08:11:19.087939Z",
     "iopub.status.idle": "2022-12-15T08:11:19.169894Z",
     "shell.execute_reply": "2022-12-15T08:11:19.168995Z",
     "shell.execute_reply.started": "2022-12-15T08:11:19.088706Z"
    },
    "id": "NW58tJe6IJMf",
    "outputId": "9e0e4dde-20e1-4e17-8fc9-f17c68d4f549"
   },
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "    extract=True)\n",
    "\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:19.172594Z",
     "iopub.status.busy": "2022-12-15T08:11:19.172212Z",
     "iopub.status.idle": "2022-12-15T08:11:19.181095Z",
     "shell.execute_reply": "2022-12-15T08:11:19.178522Z",
     "shell.execute_reply.started": "2022-12-15T08:11:19.172554Z"
    },
    "id": "zPvcVzCGIJMg"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:19.183152Z",
     "iopub.status.busy": "2022-12-15T08:11:19.182781Z",
     "iopub.status.idle": "2022-12-15T08:11:19.208014Z",
     "shell.execute_reply": "2022-12-15T08:11:19.207065Z",
     "shell.execute_reply.started": "2022-12-15T08:11:19.183117Z"
    },
    "id": "oj6hQX8VIJMh"
   },
   "outputs": [],
   "source": [
    "# 1. Remove the accents\n",
    "# 2. Clean the sentences\n",
    "# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n",
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
    "\n",
    "    return zip(*word_pairs)\n",
    "\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer\n",
    "\n",
    "\n",
    "def load_dataset(path, num_examples=None):\n",
    "    # creating cleaned input, output pairs\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:19.211022Z",
     "iopub.status.busy": "2022-12-15T08:11:19.210522Z",
     "iopub.status.idle": "2022-12-15T08:11:22.978799Z",
     "shell.execute_reply": "2022-12-15T08:11:22.977875Z",
     "shell.execute_reply.started": "2022-12-15T08:11:19.210988Z"
    },
    "id": "vNoYGAluIJMh",
    "outputId": "8666ae19-7266-491e-ace3-a52c029a0c44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 16\n"
     ]
    }
   ],
   "source": [
    "# we will be using the first 50000 examples from the dataset\n",
    "num_examples = 50000\n",
    "\n",
    "# using the preprocess module to preprocess the data\n",
    "input_tensor, target_tensor, inp_lang, tar_lang = load_dataset(path_to_file, num_examples) \n",
    "max_length_tar, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "print(max_length_tar, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:22.983387Z",
     "iopub.status.busy": "2022-12-15T08:11:22.982747Z",
     "iopub.status.idle": "2022-12-15T08:11:23.008931Z",
     "shell.execute_reply": "2022-12-15T08:11:23.008065Z",
     "shell.execute_reply.started": "2022-12-15T08:11:22.983349Z"
    },
    "id": "dmp9KFQbIJMi",
    "outputId": "d54b1fcd-9127-46a2-fcfc-288228466d95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.015236Z",
     "iopub.status.busy": "2022-12-15T08:11:23.012742Z",
     "iopub.status.idle": "2022-12-15T08:11:23.021613Z",
     "shell.execute_reply": "2022-12-15T08:11:23.020752Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.015199Z"
    },
    "id": "iaZlwZ4mIJMi"
   },
   "outputs": [],
   "source": [
    "#This function is to just display how the data is encoded\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.027567Z",
     "iopub.status.busy": "2022-12-15T08:11:23.027046Z",
     "iopub.status.idle": "2022-12-15T08:11:23.039014Z",
     "shell.execute_reply": "2022-12-15T08:11:23.037886Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.027534Z"
    },
    "id": "Q8OmeAcLIJMi",
    "outputId": "067e0756-6234-41c0-84d9-d958c10825c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "7 ----> no\n",
      "488 ----> seas\n",
      "75 ----> tan\n",
      "346 ----> duro\n",
      "150 ----> conmigo\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "28 ----> don\n",
      "12 ----> t\n",
      "37 ----> be\n",
      "88 ----> so\n",
      "229 ----> hard\n",
      "43 ----> on\n",
      "16 ----> me\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[1])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(tar_lang, target_tensor_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mWGLw31IJMj"
   },
   "source": [
    "__Now we will create a TensorFlow dataset object so that we can easily access it while training and then divide it into batches.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.041365Z",
     "iopub.status.busy": "2022-12-15T08:11:23.040422Z",
     "iopub.status.idle": "2022-12-15T08:11:23.053714Z",
     "shell.execute_reply": "2022-12-15T08:11:23.052780Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.041331Z"
    },
    "id": "BeclGUNdIJMj",
    "outputId": "8f140aa9-e37c-4448-de1b-4dabfc5834d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_tensor_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.058519Z",
     "iopub.status.busy": "2022-12-15T08:11:23.057825Z",
     "iopub.status.idle": "2022-12-15T08:11:23.070287Z",
     "shell.execute_reply": "2022-12-15T08:11:23.069410Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.058486Z"
    },
    "id": "AZtdTAEIIJMk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-24 22:14:11.958995: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-24 22:14:11.961898: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "#Size of buffer while shuffling the data\n",
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "\n",
    "#Size of the embedding layer\n",
    "embedding_dim = 256\n",
    "# No. of hidden units in the GRU\n",
    "units = 1024\n",
    "\n",
    "#Size of the Vocabulary at encoder (i.e., Spanish)\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "#Size of the Vocabulary at Decoder (i.e., English)\n",
    "vocab_tar_size = len(tar_lang.word_index)+1\n",
    "\n",
    "# Creating the Tensorflow Dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.077555Z",
     "iopub.status.busy": "2022-12-15T08:11:23.074957Z",
     "iopub.status.idle": "2022-12-15T08:11:23.250654Z",
     "shell.execute_reply": "2022-12-15T08:11:23.249728Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.077521Z"
    },
    "id": "hVPHX9CGIJMl",
    "outputId": "515e3e4f-5d25-4ac6-eacd-49557e08611c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 12]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_inp_batch, example_tar_batch = next(iter(dataset))\n",
    "example_inp_batch.shape, example_tar_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0coMviOIJMl"
   },
   "source": [
    "## __Creating the Model__\n",
    "The model we will be building is an encoder-decoder model with an extension of attention.\n",
    "1. __Encoder__:-\n",
    "This part of the model uses the simplest type of architecture. We will be using an Embedding Layer followed by GRUs. We will be using Custom Keras layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.256742Z",
     "iopub.status.busy": "2022-12-15T08:11:23.254527Z",
     "iopub.status.idle": "2022-12-15T08:11:23.268535Z",
     "shell.execute_reply": "2022-12-15T08:11:23.267480Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.256705Z"
    },
    "id": "fSVxvVmGIJMm"
   },
   "outputs": [],
   "source": [
    "# Creating a Custom Layer\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        '''\n",
    "        vocab_size : Size of the Vocabulary\n",
    "        embedding_dim : Size of the Embedding layer\n",
    "        enc_units : No. of Hidden Units in the GRU\n",
    "        batch_sz : Batch Size\n",
    "        '''\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Embedding Layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # GRU\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units, \n",
    "                                      return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "    # Function that will create the model with the layers\n",
    "    def call(self, inp, hidden):\n",
    "        '''\n",
    "        inp: input to the model.i.e., vectorized form of the spanish sentence\n",
    "        hidden: intial hidden_state of the gru.\n",
    "        '''\n",
    "        x = self.embedding(inp)\n",
    "        output, state = self.gru(x, initial_state=hidden)\n",
    "        return output, state\n",
    "    \n",
    "    # Function to initialize the hidden state of the gru\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros([self.batch_size, self.enc_units])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkwMdc8OIJMn"
   },
   "source": [
    "This will create the encoder Layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.271433Z",
     "iopub.status.busy": "2022-12-15T08:11:23.270572Z",
     "iopub.status.idle": "2022-12-15T08:11:23.285432Z",
     "shell.execute_reply": "2022-12-15T08:11:23.284376Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.271397Z"
    },
    "id": "w68OtCccIJMn"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W8j2YN5QIJMo"
   },
   "source": [
    "The way this model works is that we input a sentence in the Spanish language into the encoder part. The encoder RNN encodes the sentence and passes it to the decoder RNN which outputs its English Translation. For better results, we use an embedding layer at the encoder part to embed the input sentence. \n",
    "\n",
    "We can use the simple encoder and decoder model for our task but these types of architectures don’t work well with long sentences. We may be able to get pretty good results with short sentences but as the length of the sentences increases the accuracy of the model decreases. So to overcome this issue we will be using __Attention__.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "afzLjpI1IJMo"
   },
   "source": [
    "2.__Bahdanau Attention__ :-\n",
    "We know that RNNs are used when we want to account for previous information also. Such that an RNN takes into account the previous information to make a decision. But as the length of the sequence increases the impact of the starting words of the sentence starts decreasing due to the effect called Vanishing Gradients.\n",
    "\n",
    "What it will do is learn how to pay attention to different parts of the sentence while making a decision.\n",
    "These ideas are useful while making a Translator model because the model can easily decide on which part of the sentence to focus on while translating.\n",
    "\n",
    "![image.png](attachment:b18d8b21-9276-43eb-970d-db63013ea1cf.png)\n",
    "\n",
    "A: The output of all the GRUs is collected at this point. The output will be of the shape (batch_size,length_of_sequence,dictionary_size).\n",
    "\n",
    "B: The output of the hidden state is collected at this point. Its shape is (batch_size,hidden_units). It is a 2-D vector but we have to convert it into 3-D to work with the ‘A’ part. So we will just expand the dimensions i.e., (batch_size,1,hidden_units).\n",
    "\n",
    "STEP-1: After this, both A and B are passed into a dense layer with hidden units “h_u” that will convert A into (batch_size,length_of_sequence,h_u) and B into (batch_size,1,h_u). After this, both A and B are added together and we get the shape (batch_size,length_of_sequence,h_u). This vector is then again passed into a dense layer with only one perceptron so we get the final output in the shape (batch_size,length_of_sequence,1).\n",
    "\n",
    "STEP-2: After this, there is a softmax layer which gives us the attention weights. These attention weights store information about how much attention each word should get.\n",
    "\n",
    "STEP-3: These attention weights are then multiplied with the original output of the encoder “The output of all the GRUs” and then added along the “length_of_sequence” axis to get a Context Vector that can be used at the decoder to generate the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.287636Z",
     "iopub.status.busy": "2022-12-15T08:11:23.287006Z",
     "iopub.status.idle": "2022-12-15T08:11:23.299310Z",
     "shell.execute_reply": "2022-12-15T08:11:23.298401Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.287600Z"
    },
    "id": "CJExkfgtIJMp"
   },
   "outputs": [],
   "source": [
    "# Creating a Custom layer\n",
    "class Attention(tf.keras.layers.Layer):\n",
    "    def __init__(self,units):\n",
    "        super(Attention, self).__init__()\n",
    "        '''units: Number of hidden units, represented as \"h_u\" '''\n",
    "        \n",
    "        self.units = units\n",
    "        self.W1 = tf.keras.layers.Dense(self.units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    #funtion that will create the model with layers\n",
    "    def call(self, query, values):\n",
    "        '''\n",
    "        query : The hidden state of the GRUs \"refer to B\"\n",
    "        values: The output of the GRUs \"refer to A\"\n",
    "        '''\n",
    "        \n",
    "        #To convert the query from 2-D to 3D\n",
    "        query_expanded = tf.expand_dims(query, 1)\n",
    "        \n",
    "        \n",
    "        #Step:1 \n",
    "        '''Both query and Values are passed into a dense layer with units \"h_u\"\n",
    "        and,added together,after that a tanh activation is applied and finally\n",
    "        the result is passed into a dense layer with one neuron. '''\n",
    "        score = self.V(tf.keras.activations.tanh(\n",
    "                                                self.W1(query_expanded)+\n",
    "                                                self.W2(values)\n",
    "                                                ))\n",
    "        \n",
    "        #Step:2\n",
    "        attention_weights = tf.keras.activations.softmax(score, 1)\n",
    "        \n",
    "        #Step:3\n",
    "        context_vector = attention_weights*values\n",
    "        context_vector = tf.reduce_sum(context_vector, 1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.301749Z",
     "iopub.status.busy": "2022-12-15T08:11:23.301013Z",
     "iopub.status.idle": "2022-12-15T08:11:23.320470Z",
     "shell.execute_reply": "2022-12-15T08:11:23.319869Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.301714Z"
    },
    "id": "yMkRWd0tIJMp"
   },
   "outputs": [],
   "source": [
    "# This will create the Attention Layer:\n",
    "attention_layer = Attention(10)         #h_u = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4ciE57UIJMp"
   },
   "source": [
    "3.__Decoder__:-\n",
    "Now we have to create the decoder model that will translate the Encoded Text into English. The Decoder is the same as the Encoder but with a few changes. The Behaviour of the Decoder model will Change depending on whether it is in the “training process” or “testing process”. First, let us focus on the training part. As mentioned in the preprocessing step, each sentence starts with a <start> token. So irrespective of whether we are training or testing, the first input to the decoder model will be a start token. Consider the Diagram Below:-\n",
    "    ![image.png](attachment:c235e79c-3b17-4048-b2eb-0a330adf3667.png)\n",
    "This is the architecture of our model. You might be wondering why we are giving the decoder model an input. Remember we are considering the ‘Training Phase’ and during training, we do not use sampling rather we provide the input to the decoder so that it can predict the next word based on the true input and the attention vector. And then the error is calculated based on the word generated by the model.\n",
    "    \n",
    "1. A Spanish sentence is given to the encoder.\n",
    "2. The encoded sentence is passed to the attention Mechanism.\n",
    "3. The attention mechanism generates a vector that completely encodes the sentence.\n",
    "4. <start> token is passed to the embedding layer of the decoder.\n",
    "5. The output from the embedding layer is concatenated with the encoded vector from step 3.\n",
    "6. The concatenated vector is then passed to the GRU to predict the next word.\n",
    "7. The predicted word is compared with the true word to calculate the error.\n",
    "8. If “Estoy Bien” is input and its ground truth is “I am Fine”, then the step-7 should predict the word “I”.If there is any other word then the error will be calculated.\n",
    "9. After that, “I” is passed to the decoder that passes through the embedding layer and which then after concatenation with the vector from step 3 is passed to the GRU that tries to predict the next word. This is how the training is done. This is also called “Teacher forcing”\n",
    "10. Remember: We provide the true input to the decoder at the time of training but during testing, we don’t provide any input. We use the method of sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.325349Z",
     "iopub.status.busy": "2022-12-15T08:11:23.324604Z",
     "iopub.status.idle": "2022-12-15T08:11:23.340093Z",
     "shell.execute_reply": "2022-12-15T08:11:23.339155Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.325309Z"
    },
    "id": "6nY4RZ1wIJMq"
   },
   "outputs": [],
   "source": [
    "# Custom Layer\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        '''\n",
    "        vocab_size : Size of vocabulary for the resulting sentence (i.e.,English)\n",
    "        embedding_dim : Size of the Embedding layer\n",
    "        dec_units : No. of Hidden Units in the GRU\n",
    "        batch_size : Batch Size\n",
    "        '''\n",
    "        self.dec_units = dec_units\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #GRU\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, \n",
    "                                      return_sequences=True,\n",
    "                                      return_state = True, \n",
    "                                      recurrent_initializer = 'glorot_uniform')\n",
    "        \n",
    "        #Dense Layer\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        #Instance of attention layer\n",
    "        self.attention = Attention(self.dec_units)\n",
    "        \n",
    "    # Function to create the Layers\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        '''\n",
    "        x :Input to the decoder\n",
    "        hidden: Hidden state of the GRU\n",
    "        enc_output: Output from the encoder layer (not attention layer)\n",
    "        '''\n",
    "\n",
    "        '''combined the attention layer with the decoder layer.So that the attention \n",
    "        is calculated at the decoder.'''\n",
    "        \n",
    "        #Calculate the Attention Vector\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        #Embedding Layer\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        #Concatenate the attention vector with x\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        #Reshaping the output to (Batch_size, Vocab_dims)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        #passing it through a Dense Layer\n",
    "        x = self.dense(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.341990Z",
     "iopub.status.busy": "2022-12-15T08:11:23.341553Z",
     "iopub.status.idle": "2022-12-15T08:11:23.364368Z",
     "shell.execute_reply": "2022-12-15T08:11:23.363556Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.341958Z"
    },
    "id": "wcpcYjnnIJMq"
   },
   "outputs": [],
   "source": [
    "# Creating the Decoding Layer:\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFnRB54BIJMr"
   },
   "source": [
    "### Training the Model:\n",
    "We will be using Adam Optimizer and the SparseCategoricalCrossEntropy loss. we are providing the input as words represented in integers but the output from the decoder is not a single integer instead it is a vector of size equal to the dictionary size. We choose the index of the word from the outputted dictionary such that it has the highest probability. When calculating the loss we have to deal with the whole dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.366291Z",
     "iopub.status.busy": "2022-12-15T08:11:23.365908Z",
     "iopub.status.idle": "2022-12-15T08:11:23.373059Z",
     "shell.execute_reply": "2022-12-15T08:11:23.371919Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.366258Z"
    },
    "id": "qp_DI2dDIJMr"
   },
   "outputs": [],
   "source": [
    "#Optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Loss Object\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "'''We will be taking bulks of data from the dataset while training\n",
    "This function just uses the Loss Object to deal with a bulk of\n",
    "data,and calculate loss for the whole bulk'''\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_a6YK11EIJMr"
   },
   "source": [
    "Creating Checkpoints and Storing Model Progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:11:23.375119Z",
     "iopub.status.busy": "2022-12-15T08:11:23.374766Z",
     "iopub.status.idle": "2022-12-15T08:11:23.385554Z",
     "shell.execute_reply": "2022-12-15T08:11:23.384632Z",
     "shell.execute_reply.started": "2022-12-15T08:11:23.375086Z"
    },
    "id": "LVGE5Pe1IJMs"
   },
   "outputs": [],
   "source": [
    "# Path where the checkpoint is to be stored\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "# Storing the index of the checkpoint\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "\n",
    "# Checkpoint object that will store the state of each layer\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                encoder=encoder, \n",
    "                                decoder = decoder)\n",
    "\n",
    "# we combined the Attention layer with the decoder layer so we don't\n",
    "#have to save it explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLHh3XcAIJMs"
   },
   "source": [
    "1. Define the number of Epochs and Initialize the hidden state of the Encoder model.\n",
    "2. Initialize total loss(initially zero).\n",
    "3. Get the bulk of data from the dataset.\n",
    "4. Pass this data to the encoder. The encoder will return the encoded sentences and its hidden state.\n",
    "5. Pass the hidden state and the encoded outputt to the decoder along with the true output. i.e., original translation.\n",
    "6. Pass the hidden state and encoded output to the attention mechanism to generate a new encoding vector.\n",
    "7. Pass the 'start' token to the decoder. Note: As we are using batches of data to train so we have to pass 'start' token to every batch. This can be done by duplicating 'start' equal to the sie of the batch.\n",
    "    \n",
    "8. Step 6 and 7 are done simultaneously in the decoder layer. Then the decoder layer generates predictions and its hidden state.\n",
    "9. Calculate the error on the predictions. As we calculated loss for the whole batch, we have to account for it by dividing the loss by the length of the sequence. It is just a convention.\n",
    "10. Now, get all the trainable variables., weights and calculate the gradient(Derivative) of those variables with respect to loss. \n",
    "\n",
    "Finally, update the variables by applying gradients.\n",
    "That’s it. This is how you can train a Neural Machine Translation Model. For convenience, We have divided this into two functions. One for steps 1–3 and the other for 4–10.\n",
    "    \n",
    "Function for steps 4–10:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T08:12:06.542901Z",
     "iopub.status.busy": "2022-12-15T08:12:06.542512Z",
     "iopub.status.idle": "2022-12-15T08:12:06.552190Z",
     "shell.execute_reply": "2022-12-15T08:12:06.551068Z",
     "shell.execute_reply.started": "2022-12-15T08:12:06.542864Z"
    },
    "id": "aTuwJ8xeIJMs"
   },
   "outputs": [],
   "source": [
    "def training_step(inp, targ, enc_hidden_state):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Step: 4\n",
    "        enc_output, enc_hidden_state = encoder(inp, enc_hidden_state)\n",
    "        dec_hidden_state = enc_hidden_state\n",
    "        \n",
    "        #Step: 7\n",
    "        dec_input = tf.expand_dims([tar_lang.word_index['<start>']]*BATCH_SIZE, 1)\n",
    "        #We have to concatenate the decoder input with the attention vector\n",
    "        #So we have to convert it into 2-D by expanding Dims.\n",
    "        \n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, dec_hidden_state, _  = decoder(dec_input, dec_hidden_state, enc_output)\n",
    "            \n",
    "            \n",
    "            #Increment the loss,targ[:,t] means we are calculating loss for the \n",
    "            #t'th word in all batches at the same time.(BULK!)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss/int(targ.shape[1]))\n",
    "    \n",
    "    # Get all the trainable variables\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    #Calculate the Derivative\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    #Apply the gradients\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BiqZK1gfIJMs"
   },
   "source": [
    "__Training the model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T08:12:13.853916Z",
     "iopub.status.busy": "2022-12-15T08:12:13.853547Z",
     "iopub.status.idle": "2022-12-15T08:30:41.848131Z",
     "shell.execute_reply": "2022-12-15T08:30:41.847075Z",
     "shell.execute_reply.started": "2022-12-15T08:12:13.853883Z"
    },
    "id": "6XGmcX6aIJMs",
    "outputId": "06261a1c-efe3-4b3f-d160-2ea0ff259b54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.4711\n",
      "Epoch 1 Batch 100 Loss 2.2360\n"
     ]
    }
   ],
   "source": [
    "Epochs = 10\n",
    "for epoch in range(Epochs):\n",
    "    # to calculate the time take for each epoch\n",
    "    start = time.time()\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Taking the batches of data\n",
    "    for (batch, (inp, tar)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = training_step(inp, tar, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        # Printing stats after 100 batches\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "            \n",
    "        #saving (checkpoint) the model after every 2 epochs\n",
    "        \n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix= checkpoint_prefix)\n",
    "\n",
    "    # Printing some stats:\n",
    "    print('Epoch {}, Loss{:.4f}'.format(epoch+1, \n",
    "                                       total_loss/steps_per_epoch))\n",
    "    print('Time taken for 1 Epoch {} sec\\n'.format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LxcR85vOIJMt"
   },
   "source": [
    "## Evaluating and Testing the Model\n",
    " During Training, we provided the decoder with the actual output so that it can learn. But now we want to convert over Spanish text into English. We don’t have the English translation. We want the model to provide us with English translation. But the decoder model requires the previous word to predict the next word. This is where sampling comes into place.\n",
    " ![image.png](attachment:81890ea7-9f54-4b5a-8d9c-8d928f788894.png)\n",
    " What we are doing is at first we are passing the <start> token to the decoder. It then predicts the next word. This word is no more used for calculating error. In fact, this word will be used as input to the next GRU (or Time Step). At the next time step, the model generated another word. We keep track of all the words by appending it to an empty string. So when do we stop generating new words?\n",
    "When the maximum length of the output sequence is achieved or As soon as the model predicts the next word as <end> we will stop feeding the RNN.\n",
    "    \n",
    "overview:-\n",
    "\n",
    "1. Input a sentence in Spanish.\n",
    "2. Preprocess the Sentence.\n",
    "3. Split the sentence into words and pad zeros at the end of the length of the sentence is less than the length accepted by the RNN.\n",
    "4. Covert it into a tensor.\n",
    "5. Initialize hidden state of encoder, feed the sentence to the encoder, and get the encoded output.\n",
    "6. Input the 'start' token in the decoder.\n",
    "7. Start a loop for predicting the translation of the given sentence word by word.\n",
    "7. At each step, the GRU will output a vector of size equal to the dictionary size. We pick the index of the highest value from the vector (which refers to the most probable value), convert it into its corresponding word using the dictionary, append the word to the output string, pass the index of that word again to the decoder model to predict the next word.SAMPLING!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T09:03:40.258357Z",
     "iopub.status.busy": "2022-12-15T09:03:40.257682Z",
     "iopub.status.idle": "2022-12-15T09:03:40.267580Z",
     "shell.execute_reply": "2022-12-15T09:03:40.266664Z",
     "shell.execute_reply.started": "2022-12-15T09:03:40.258319Z"
    },
    "id": "gB-kKAW9IJMt"
   },
   "outputs": [],
   "source": [
    "# Function to translate a sentence\n",
    "def evaluate(sentence):\n",
    "    \"This is to store the attentionn vector for plotting\"\n",
    "    attention_plot = np.zeros((max_length_tar, max_length_inp))\n",
    "    \n",
    "    #preprocessing the sentence. Steps 2, 3, 4\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], \n",
    "                                                          maxlen=max_length_inp,\n",
    "                                                          padding='post')\n",
    "    \n",
    "    #Step:4\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    #Creating a string to store the translated sentence\n",
    "    result = ''\n",
    "    \n",
    "    #Step: 5\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "    dec_hidden = enc_hidden\n",
    "    \n",
    "    #Step: 6\n",
    "    dec_input = tf.expand_dims([tar_lang.word_index['<start>']], 0)\n",
    "    \n",
    "    \n",
    "    #Step: 7\n",
    "    for t in range(max_length_tar):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, \n",
    "                                                            dec_hidden, \n",
    "                                                            enc_out)\n",
    "        \n",
    "        #Storing the attention weights to plot later\n",
    "        attention_weights = tf.reshape(attention_weights, (-1))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "        \n",
    "        \n",
    "        #Step:8\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "#         if tar_lang.index_word[predicted_id] != '<end>':\n",
    "        result += tar_lang.index_word[predicted_id] + ' '\n",
    "        \n",
    "        \n",
    "        if tar_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # the predicted Id is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "    #Return the original sentence, Translated Sentence and the history if attention\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T09:04:24.147369Z",
     "iopub.status.busy": "2022-12-15T09:04:24.147017Z",
     "iopub.status.idle": "2022-12-15T09:04:24.228119Z",
     "shell.execute_reply": "2022-12-15T09:04:24.227204Z",
     "shell.execute_reply.started": "2022-12-15T09:04:24.147338Z"
    },
    "id": "TvSJUkmeIJMt",
    "outputId": "f2269949-6038-4359-80bd-f8c01a31fc90"
   },
   "outputs": [],
   "source": [
    "sentence = u'Esta es la primera vez que te conocí'\n",
    "translation, _, _ = evaluate(sentence)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFinl8goIJMt"
   },
   "source": [
    "## Visualizing Resut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T09:08:36.676438Z",
     "iopub.status.busy": "2022-12-15T09:08:36.676091Z",
     "iopub.status.idle": "2022-12-15T09:08:36.686227Z",
     "shell.execute_reply": "2022-12-15T09:08:36.685242Z",
     "shell.execute_reply.started": "2022-12-15T09:08:36.676408Z"
    },
    "id": "F2JuhULsIJMu"
   },
   "outputs": [],
   "source": [
    "from pylab import *\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-15T09:07:17.628997Z",
     "iopub.status.busy": "2022-12-15T09:07:17.628320Z",
     "iopub.status.idle": "2022-12-15T09:07:17.634689Z",
     "shell.execute_reply": "2022-12-15T09:07:17.633721Z",
     "shell.execute_reply.started": "2022-12-15T09:07:17.628957Z"
    },
    "id": "QAdrLMsjIJMu"
   },
   "outputs": [],
   "source": [
    "def translate_sentence(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T09:06:49.347597Z",
     "iopub.status.busy": "2022-12-15T09:06:49.346932Z",
     "iopub.status.idle": "2022-12-15T09:06:49.585448Z",
     "shell.execute_reply": "2022-12-15T09:06:49.584539Z",
     "shell.execute_reply.started": "2022-12-15T09:06:49.347559Z"
    },
    "id": "Raw3rkgCIJMu",
    "outputId": "1990082a-b55d-4c8c-bfb2-2205969d8786"
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T09:08:38.695536Z",
     "iopub.status.busy": "2022-12-15T09:08:38.694633Z",
     "iopub.status.idle": "2022-12-15T09:08:39.039588Z",
     "shell.execute_reply": "2022-12-15T09:08:39.038201Z",
     "shell.execute_reply.started": "2022-12-15T09:08:38.695490Z"
    },
    "id": "e6xfy_TpIJMu",
    "outputId": "3508e958-9c08-4d60-e1fe-9c026789d2b3"
   },
   "outputs": [],
   "source": [
    "translate_sentence(u'hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T09:08:58.536825Z",
     "iopub.status.busy": "2022-12-15T09:08:58.536456Z",
     "iopub.status.idle": "2022-12-15T09:08:58.895372Z",
     "shell.execute_reply": "2022-12-15T09:08:58.894030Z",
     "shell.execute_reply.started": "2022-12-15T09:08:58.536793Z"
    },
    "id": "CJG4gU6pIJMu",
    "outputId": "50bceb6d-f814-4793-b064-f00bbee715a2"
   },
   "outputs": [],
   "source": [
    "\n",
    "translate_sentence(u'¿todavia estan en casa?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 675
    },
    "execution": {
     "iopub.execute_input": "2022-12-15T09:09:43.016776Z",
     "iopub.status.busy": "2022-12-15T09:09:43.016421Z",
     "iopub.status.idle": "2022-12-15T09:09:43.348698Z",
     "shell.execute_reply": "2022-12-15T09:09:43.347440Z",
     "shell.execute_reply.started": "2022-12-15T09:09:43.016744Z"
    },
    "id": "skG6vpPqIJMu",
    "outputId": "56d9337e-5428-4cb2-c82a-3891f04bc2d2"
   },
   "outputs": [],
   "source": [
    "translate_sentence('Esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4R7HIwtfWJYx"
   },
   "outputs": [],
   "source": [
    "import gradio as gr            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPvgcB_DWIFJ"
   },
   "outputs": [],
   "source": [
    "interface = gr.Interface(fn=evaluate, \n",
    "                         inputs=gr.inputs.Textbox(lines=2, placeholder='Text to translate'),\n",
    "                        outputs='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 614
    },
    "id": "L34Aa8ZhWZRC",
    "outputId": "4b068c30-60ef-4ac3-94ae-2078c23a08ac"
   },
   "outputs": [],
   "source": [
    "interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38JVVaHgIJMu"
   },
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
